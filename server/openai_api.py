"""OpenAI API helper functions module."""

import json
import os
import requests
import tiktoken


__author__ = "Adam Coscia"
__license__ = "MIT"
__version__ = "0.1.0"
__email__ = "acoscia125@gmail.com"


def get_num_tokens_from_message(messages, model_checkpoint):
    """Returns the number of tokens used by a list of messages.

    Uses `tiktoken` tokenizer.

    See:
    - <https://platform.openai.com/docs/guides/text-generation/managing-tokens>
    - <https://github.com/openai/tiktoken>
    """
    try:
        encoding = tiktoken.get_encoding("cl100k_base")
    except KeyError:
        raise NotImplementedError(
            f"""get_num_tokens_from_message() is not presently implemented for model {model_checkpoint}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens."""
        )

    num_tokens = 0
    for message in messages:
        num_tokens += 4  # every message follows <im_start>{role/name}\n{content}<im_end>\n
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
            if key == "name":  # if there's a name, the role is omitted
                num_tokens += -1  # role is always required and always 1 token
    num_tokens += 2  # every reply is primed with <im_start>assistant

    return num_tokens


def format_chat_messages(model_checkpoint, documents, doc_sep, user_instructions, task_prompt_formatter):
    """Formats `documents`, `doc_sep`, and list of `user_instructions` into OpenAI messages formatted prompt using `task_prompt_formatter` function:

    ```
    [
        {
            "role" "system",
            "content", "...",
        },
        {
            "role" "user",
            "content", "...",
        },
    ]
    ```

    Uses encoders to track number of tokens being used as input, to determine how to format documents and how many tokens are left for output.

    Returns list of `messages` formatted for OpenAI API and `max_tokens` model is allowed to output.

    See: <https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models>

    ### Model settings

    Context window determines entire API call, including both input and output tokens

    - `gpt-3.5-turbo`
      - NOTE: `gpt-3.5-turbo` currently points to `gpt-3.5-turbo-0125` (03/26/2024)
      - Context window: 16385 tokens
      - Max tokens: 4096 tokens

    See: <https://platform.openai.com/docs/models/overview>
    """
    # model specific settings
    min_output_tokens = 500  # reserve minimum number tokens to generate
    max_output_tokens = 500  # tokens generated by models, use to limit input tokens
    context_window = 1024  # total tokens incl. both input and output

    if model_checkpoint == "gpt-4.1":
        max_output_tokens = 32768
        context_window = 1047576
    elif model_checkpoint == "gpt-3.5-turbo":
        max_output_tokens = 4096
        context_window = 16385

    print(f"context window: {context_window}")

    # get tokenizer
    try:
        encoding = tiktoken.encoding_for_model(model_checkpoint)
    except KeyError:
        print("Warning: model not found. Using cl100k_base encoding.")
        encoding = tiktoken.get_encoding("cl100k_base")

    # determine max tokens per doc
    doc_sep_tokens = encoding.encode(doc_sep)  # encode separator string as tokens
    message_template = task_prompt_formatter(doc_sep, "", user_instructions)  # get messages without documents added
    num_message_tokens = get_num_tokens_from_message(message_template, model_checkpoint)  # reserve tokens for messages
    reserved_tokens = (
        context_window  # total tokens available, less:
        - min_output_tokens  # - tokens reserved for the output
        - (len(doc_sep_tokens) * (len(documents) - 1))  # - tokens used by doc_sep between each doc
        - num_message_tokens  # - tokens used by messages formatting and prompt
    )
    max_tokens_per_doc = reserved_tokens // len(documents)  # split tokens remaining evenly between documents

    # create document prompt within limits of context window and output tokens
    prompt_tokens = []
    for i, doc in enumerate(documents):
        doc = " ".join(doc.replace("\n", " ").split())  # remove newlines, the model hates 'em
        tokens = encoding.encode(doc)  # encode document
        new_tokens = tokens[0:max_tokens_per_doc]  # truncate up to max tokens per document
        prompt_tokens.extend(new_tokens)  # add tokens to prompt_tokens
        if i < len(documents) - 1:
            prompt_tokens.extend(doc_sep_tokens)  # add separator token between documents
    doc_prompt = encoding.decode(prompt_tokens)  # decode tokenized documents and separators

    print(f"doc prompt tokens: {len(prompt_tokens)}")

    # create task-specific OpenAI API formatted message by inserting document prompt
    messages = task_prompt_formatter(doc_sep, doc_prompt, user_instructions)

    # check how many input tokens the prompt will use, including message formatting
    estimated_input_tokens_used = get_num_tokens_from_message(messages, model_checkpoint)
    print(f"total input tokens: {estimated_input_tokens_used}")

    # set max tokens to generate based on context window and input tokens
    max_tokens = min(context_window - estimated_input_tokens_used, max_output_tokens)
    print(f"max output tokens: {max_tokens}")

    return messages, max_tokens


def request_chat_endpoint(model_checkpoint, messages, max_tokens, endpoint_params, seed=None):
    """Makes a request to OpenAI chat API endpoint.

    - Saves running count of input / output / total tokens used in OpenAI calls.
      - OpenAI charges per number of tokens used and charges differently for input vs output tokens.

    See: <https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models>

    ### Options

    - `frequency_penalty`: Penalizes tokens based on their frequency, reducing repetition.
    - `logit_bias`: Modifies likelihood of specified tokens with bias values.
    - `logprobs`: Returns log probabilities of output tokens if true.
    - `top_logprobs`: Specifies the number of most likely tokens to return at each position.
    - `max_tokens`: Sets the maximum number of generated tokens in chat completion.
    - `n`: Generates a specified number of chat completion choices for each input.
    - `presence_penalty`: Penalizes new tokens based on their presence in the text.
    - `response_format`: Specifies the output format, e.g., JSON mode.
    - `seed`: Ensures deterministic sampling with a specified seed.
    - `stop`: Specifies up to 4 sequences where the API should stop generating tokens.
    - `stream`: Sends partial message deltas as tokens become available.
    - `temperature`: Sets the sampling temperature between 0 and 2.
    - `top_p`: Uses nucleus sampling; considers tokens with top_p probability mass.
    - `tools`: Lists functions the model may call.
    - `tool_choice`: Controls the model's function calls (none/auto/function).
    - `user`: Unique identifier for end-user monitoring and abuse detection.

    See: <https://platform.openai.com/docs/api-reference/chat/create>
    """
    # set up request parameters
    url = "https://api.openai.com/v1/chat/completions"
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {endpoint_params['API_TOKEN']}"}
    data = {
        "model": model_checkpoint,
        "messages": messages,
        "frequency_penalty": endpoint_params["frequency_penalty"],
        "max_tokens": max_tokens,  # set to not go over max context length of model
        "n": 1,  # generate a single response
        "presence_penalty": endpoint_params["presence_penalty"],
        "seed": seed if seed is not None else seed,  # try to guarantee deterministic outputs
        "temperature": endpoint_params["temperature"],
        "top_p": endpoint_params["top_p"],
    }

    # make a POST request to get summary
    r = requests.post(url, headers=headers, data=json.dumps(data))
    status = r.status_code
    print(f"API response code: {status}")

    # response ok, proceed as normal
    response = r.json()

    # save copy of response
    fp = os.path.join(".", "data", "usage", "response.json")
    with open(fp, "w") as f:
        f.write(json.dumps(response))

    if status == 200:
        # keep track of how many tokens have been used so far, for each model checkpoint
        model_used = response["model"]

        try:
            fp = os.path.join(".", "data", "usage", f"tokens_used_{model_used}.json")
            with open(fp, "r") as f:
                tokens_used = json.load(f)
        except IOError:
            tokens_used = {"input": 0, "output": 0, "total": 0}

        input_tokens_used = response["usage"]["prompt_tokens"]
        output_tokens_used = response["usage"]["completion_tokens"]
        total_tokens_used = response["usage"]["total_tokens"]

        tokens_used["input"] += input_tokens_used
        tokens_used["output"] += output_tokens_used
        tokens_used["total"] += total_tokens_used

        fp = os.path.join(".", "data", "usage", f"tokens_used_{model_used}.json")
        with open(fp, "w") as f:
            json.dump(tokens_used, f, indent=2)

        print(f"total tokens used: {total_tokens_used}")

        return status, response, input_tokens_used, output_tokens_used
    else:
        # unknown response, return entire response
        return status, response, None, None


def request_embedding_endpoint(model_checkpoint, user_query, endpoint_params):
    """Makes a request to OpenAI embedding API endpoint.

    - Saves running count of input / output / total tokens used in OpenAI calls.
      - OpenAI charges per number of tokens used and charges differently for input vs output tokens.

    See: <https://platform.openai.com/docs/guides/embeddings>

    ### Options

    - `input`: Input text to embed, encoded as a string or array of tokens.
      - To embed multiple inputs in a single request, pass an array of strings or array of token arrays.
      - The input must not exceed the max input tokens for the model (8192 tokens for text-embedding-ada-002), cannot be an empty string, and any array must be 2048 dimensions or less.
    - `model`: ID of the model to use.
    - `encoding_format`: The format to return the embeddings in. Can be either `float` or `base64`.
    - `dimensions`: The number of dimensions the resulting output embeddings should have.
    - `user`: Unique identifier for end-user monitoring and abuse detection.

    See: <https://platform.openai.com/docs/api-reference/embeddings/create>
    """
    # set up request parameters
    url = "https://api.openai.com/v1/embeddings"
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {endpoint_params['API_TOKEN']}"}
    data = {
        "input": user_query,
        "model": model_checkpoint,
        "encoding_format": endpoint_params["format"],
    }
    if endpoint_params["dimensions"] is not None:
        data["dimensions"] = endpoint_params["dimensions"]  # not using default setting

    # make a POST request to get summary
    r = requests.post(url, headers=headers, data=json.dumps(data))
    status = r.status_code
    print(f"API response code: {status}")

    # response ok, proceed as normal
    response = r.json()

    # save copy of response
    fp = os.path.join(".", "data", "usage", "response.json")
    with open(fp, "w") as f:
        f.write(json.dumps(response))

    if status == 200:
        # keep track of how many tokens have been used so far, for each model checkpoint
        model_used = response["model"]

        try:
            fp = os.path.join(".", "data", "usage", f"tokens_used_{model_used}.json")
            with open(fp, "r") as f:
                tokens_used = json.load(f)
        except IOError:
            tokens_used = {"input": 0, "output": 0, "total": 0}

        input_tokens_used = response["usage"]["prompt_tokens"]
        total_tokens_used = response["usage"]["total_tokens"]

        tokens_used["input"] += input_tokens_used
        tokens_used["total"] += total_tokens_used

        fp = os.path.join(".", "data", "usage", f"tokens_used_{model_used}.json")
        with open(fp, "w") as f:
            json.dump(tokens_used, f, indent=2)

        print(f"total tokens used: {total_tokens_used}")

        return status, response, input_tokens_used
    else:
        # unknown response, return entire response
        return status, response, None
